<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autonomous Vehicle RL Project</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</head>
<body>
    <header>
        <nav>
            <div class="nav-content">
                <h1>Self-Driving Cars</h1>
                <ul>
                    <li><a href="#overview">Overview</a></li>
                    <li><a href="#goals">Goals</a></li>
                    <li><a href="#technical">Technical Details</a></li>
                    <li class="dropdown">
                        <a href="#implementation">Implementation</a>
                        <ul class="dropdown-content">
                            <li><a href="#implementation">Environment Design</a></li>
                            <li><a href="#implementation">Agent Design</a></li>
                            <li><a href="#implementation">Proximal Policy Optimization</a></li>
                            <li><a href="#implementation">Reward Function Design</a></li>
                            <li><a href="#implementation">Multi-Agent Learning</a></li>
                            <li><a href="#implementation">Data Collection</a></li>
                        </ul>
                    </li>
                    <li><a href="#contact">Contact</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <main>
        <section id="hero">
            <div class="hero-content">
                <h1>Multi-Agent Reinforcement Learning for Self-Driving Cars</h1>
                <p>A sophisticated simulation environment for testing and developing autonomous driving algorithms</p>
                <a href="#overview" class="explore-button">Learn More</a>
            </div>
        </section>

        <section id="overview">
            <div class="container">
                <h2>Project Overview</h2>
                <p>This project represents an advanced exploration into autonomous vehicle navigation using reinforcement learning techniques. Through a custom-built Unity Game Engine simulation environment, I'm developing and testing sophisticated algorithms that enable autonomous vehicles to navigate complex road scenarios.</p>
                
                <div class="features">
                    <div class="feature-card">
                        <i class="fas fa-car"></i>
                        <h3>Autonomous Navigation</h3>
                        <p>Advanced algorithms for real-time decision making and control</p>
                    </div>
                    <div class="feature-card">
                        <i class="fas fa-brain"></i>
                        <h3>Reinforcement Learning</h3>
                        <p>State-of-the-art RL techniques for continuous control problems</p>
                    </div>
                    <div class="feature-card">
                        <i class="fas fa-code"></i>
                        <h3>Simulation Environment</h3>
                        <p>Modular Unity Game Engine-based testing environment</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="goals">
            <div class="container">
                <h2>Project Goals</h2>
                <div class="goals-grid">
                    <div class="goal-item">
                        <h3>Technical Understanding</h3>
                        <p>Develop comprehensive understanding of reinforcement learning techniques applied to continuous control problems</p>
                    </div>
                    <div class="goal-item">
                        <h3>Simulation Development</h3>
                        <p>Create a flexible and modular simulation environment using Unity Game Engine</p>
                    </div>
                    <div class="goal-item">
                        <h3>Algorithm Demonstration</h3>
                        <p>Demonstrate the capabilities of Multi-Agent Proximal Policy Optimization in autonomous vehicle navigation</p>
                    </div>
                    <div class="goal-item">
                        <h3>Research Foundation</h3>
                        <p>Establish a foundation for further research and development in autonomous driving</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="technical">
            <div class="container">
                <h2>Technical Stack</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <i class="fab fa-python"></i>
                        <h3>Python</h3>
                        <p>Primary programming language</p>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-gamepad"></i>
                        <h3>Unity Game Engine</h3>
                        <p>Simulation environment development</p>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-network-wired"></i>
                        <h3>PyTorch/TensorFlow</h3>
                        <p>Reinforcement learning implementation</p>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-book"></i>
                        <h3>Jupyter Notebook</h3>
                        <p>Documentation and experimentation</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="implementation">
            <div class="container">
                <h2>Implementation Details</h2>
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Environment Design</h3>
                        <p>
                            The autonomous driving simulation was built using Unity's ML-Agents toolkit to provide a controlled and flexible environment for reinforcement learning experiments. The virtual driving environment was carefully designed to balance realism with training efficiency, ensuring that the agent could learn effectively while avoiding unnecessary computational overhead.
                          </p>
                        
                          <h4 style="margin-top: 1.5rem;">Key Features:</h4>
                          <ul style="line-height: 1.6;">
                            <li>
                              <strong>Track Layouts:</strong> The environment includes multiple road configurations such as straight segments, sharp turns, intersections, and continuous loops. These variations challenge the agent's ability to generalize its learned policies across different driving conditions. In order to ensure smooth training, the agent is initially trained on the race track on the left to learn the basic behaviors required to drive. Then the pretrained agent is transfered to the urban environment featured on the left to learn how to interact with sharper turns and intersections.
                              <ul>
                                <li><img src="images/env-1.png"></li>
                                <li><img src="images/env-2.png"></li>
                              </ul>
                            </li>
                            <li>
                              <strong>Road Markings & Boundaries:</strong> Lanes are marked with distinct colors and borders to simulate real-world road constraints. Going off-road or crossing into restricted areas results in negative rewards, guiding the agent to stay within safe driving zones. When the agent enters the red areas, the agent is penalized. The green represents the checkpoints to guide the direction of the agent. The agent is awarded when reaching the correct checkpoint.
                              <ul>
                                <li><img src="images/boundaries.png"></li>
                              </ul>
                            </li>
                            <li>
                              <strong>Sensor System:</strong> The agent is equipped with a front-facing raycast sensor array that simulates lidar-like perception. These rays detect distances to walls and obstacles, feeding spatial data into the neural network to inform decision-making. Note that the checkpoints are not detected by the sensors.
                              <ul>
                                <li><img src="images/sensor.gif"></li>
                              </ul>
                            </li>
                            <li>
                              <strong>Action Space:</strong> The agent outputs continuous values for steering and throttle, mimicking real vehicle controls. This enables smoother and more realistic driving behavior compared to discrete action spaces because it can make small fluid adjustments instead of being limited to a set of actions. In a complex driving environment, having a continous action space allows the agent to make detailed adjustments when dealing with avoiding aobstacles. However, this comes with the drawback of a longer training time because of the infinite number of possible actions that the agent needs to explore and learn causing noisier updates and slower convergance.
                            </li>
                            <li>
                              <strong>Reward System:</strong> Rewards are structured to incentivize forward motion, staying in lane, and avoiding collisions. A small penalty is given for idling or veering off track, helping to shape desirable driving behaviors over time.
                            </li>
                            <li>
                              <strong>Reset Conditions:</strong> Episodes reset automatically upon collision. Episodes also reset when passing the wrong checkpoint to prevent agents from moving in the wrong direction.
                            </li>
                          </ul>
                    </div>
                </div>

                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Agent Design</h3>
                        <p>The agent's design is crucial for effective learning and decision-making in the autonomous driving environment. The agent's architecture and components are carefully designed to process environmental information and make appropriate driving decisions.</p>
                        <h4>Key Components:</h4>
                        <ul>
                            <li>
                                <strong>Controller & Physics System:</strong> The agent uses Unity's physics engine to create a Rigidbody to simulate realistic movement, acceleration, braking, and steering:
                                <ul>
                                    <li>Motor torque is applied to the rear wheels based on gas input and engine power</li>
                                    <li>Torque is calculated from the engine RPM and power curve</li>
                                    <li>Gear shifting: upshift when RPM exceeds a threshold and downshift when RPM drops below a threshold</li>
                                    <li>Brake torque is applied to all wheels</li>
                                </ul>
                            </li>
                            <li>
                                <strong>Observation Space:</strong> The agent receives a comprehensive set of observations including:
                                <ul>
                                    <li>Raycast sensor data for obstacle detection</li>
                                    <li>Global position</li>
                                    <li>Normalized rotational vector</li>
                                    <li>Velocity vector and magnitude</li>
                                    <li>Angular vector and magnitude</li>
                                    <li>Relative position to road boundaries</li>
                                    <li>Checkpoint proximity information</li>
                                </ul>
                            </li>
                            <li>
                                <strong>State Stacking & State Prediction:</strong>
                                <p>The agent's decision-making process is enhanced through a sophisticated state stacking and prediction mechanism. This approach allows the agent to maintain temporal awareness and make more informed decisions by considering both historical and predicted future states.</p>

                                <h4>State Stacking</h4>
                                <p>The state stacking mechanism concatenates the current state with the previous two states to create a temporal context for decision-making. This approach is crucial for several reasons:</p>
                                <ul>
                                    <li>The current state provides immediate information about the agent's position, velocity, and surrounding environment.</li>
                                    <li>The previous two states help the agent understand its motion dynamics, including acceleration patterns and trajectory trends.</li>
                                    <li>This temporal context enables the agent to make smoother decisions by considering how its state has evolved over time.</li>
                                </ul>
                                <p>For example, when approaching a turn, the stacked states help the agent understand its current speed, how it has been accelerating or decelerating, and what adjustments might be needed to navigate the turn smoothly.</p>

                                <h4>State Prediction</h4>
                                <p>The state prediction system uses a pretrained neural network to forecast the next two states based on the current and stacked historical states. This prediction mechanism serves several important purposes:</p>
                                <ul>
                                    <li>The pretrained network has learned the dynamics of the environment and can accurately predict how the agent's state will evolve.</li>
                                    <li>By predicting future states, the agent can anticipate potential issues before they occur, such as upcoming obstacles or the need to adjust speed for a turn.</li>
                                    <li>The predictions help the agent make more proactive decisions rather than purely reactive ones.</li>
                                </ul>
                                <p>The prediction network takes as input the current state and the stacked historical states, and outputs predictions for the next two timesteps. These predictions include expected positions, velocities, and other relevant state variables. This forward-looking capability is particularly valuable in dynamic environments where immediate reactions might be too late to avoid problems.</p>

                                <h4>Integration and Benefits</h4>
                                <p>The combination of state stacking and prediction creates a powerful decision-making framework:</p>
                                <ul>
                                    <li><strong>Enhanced Temporal Awareness:</strong> The agent maintains a clear understanding of both its past trajectory and likely future states, enabling more informed decision-making.</li>
                                    <li><strong>Proactive Planning:</strong> By predicting future states, the agent can plan its actions in advance rather than simply reacting to immediate circumstances.</li>
                                    <li><strong>Improved Stability:</strong> The temporal context helps prevent erratic behavior by considering the agent's motion history when making decisions.</li>
                                    <li><strong>Better Performance:</strong> This approach leads to smoother trajectories, more efficient navigation, and better handling of complex scenarios.</li>
                                </ul>
                                <p>For instance, when navigating through a series of turns, the agent can use its stacked states to understand its current trajectory and use the predicted states to plan the necessary adjustments for upcoming turns. This results in smoother, more efficient navigation compared to making decisions based solely on the current state.</p>
                            </li>
                            <li>
                                <strong>Neural Network Architecture:</strong>
                                <ul>
                                    <li>Input layer processes sensor and state data</li>
                                    <li>Multiple hidden layers with ReLU activation</li>
                                    <li>Output layer for continuous action values for gas, brake, and steering angle</li>
                                    <li>Dropout layers and normalization used to prevent overfitting</li>
                                    <li>Independent shared value network for advantage estimation</li>
                                </ul>
                            </li>
                        </ul>
                    </div>
                </div>
                
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Proximal Policy Optimization (PPO)</h3>
                        <p>PPO is a popular algorithm in reinforcement learning. It helps train an agent (like a robot or game player) to make good decisions by improving its strategy little by little in a stable way. PPO works well because it keeps policy updates from being too big, which can otherwise break learning.</p>
                
                        <h4>What’s the Goal in Reinforcement Learning?</h4>
                        <p>In RL, we want to find the best <strong>policy</strong>—a function that tells our agent what to do in each situation (state) to get the most reward over time.</p>
                        <p>The expected reward is written like this:</p>
                        <p>
                            \[
                            J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]
                            \]
                        </p>
                        <ul>
                            <li><strong>s</strong>: current state</li>
                            <li><strong>a</strong>: action taken</li>
                            <li><strong>r_t</strong>: reward at time <em>t</em></li>
                            <li><strong>\(\gamma\)</strong>: discount factor (makes future rewards worth slightly less)</li>
                            <li><strong>\(\pi_\theta(a|s)\)</strong>: the policy’s probability of choosing action <em>a</em> in state <em>s</em></li>
                        </ul>
                
                        <h4>Basic Policy Gradient Method</h4>
                        <p>The basic way to improve the policy is called the <strong>policy gradient</strong>. It tries to push the policy in a direction that increases expected reward. The formula looks like this:</p>
                        <p>
                            \[
                            \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot A^{\pi}(s,a) \right]
                            \]
                        </p>
                        <p>Here, \( A^{\pi}(s,a) \) is the <em>advantage</em>, which tells us how much better an action is than average at a given state.</p>
                        <p>The problem is: basic policy gradients can make changes that are too big, which makes learning unstable.</p>
                
                        <h4>PPO: A Smarter Way to Update the Policy</h4>
                
                        <h5>Step 1: Compare New and Old Policy</h5>
                        <p>PPO compares the new and old policy using this ratio:</p>
                        <p>
                            \[
                            r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
                            \]
                        </p>
                        <p>This tells us how much the new policy differs from the old one for the same action.</p>
                
                        <h5>Step 2: Clip the Objective</h5>
                        <p>Instead of letting this ratio get too big or too small, we clip it. The goal is to avoid big, risky updates.</p>
                        <p>
                            \[
                            L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
                            \]
                        </p>
                        <ul>
                            <li>\(\hat{A}_t\): advantage at time <em>t</em></li>
                            <li>\(\epsilon\): small constant (usually 0.1 or 0.2)</li>
                        </ul>
                        <p>This keeps the update close to the old policy, which makes learning more stable.</p>
                
                        <h5>Why Clipping Helps</h5>
                        <p>If the ratio becomes too large or too small (e.g., 1.5 or 0.5), clipping stops the algorithm from giving it too much weight. This keeps learning steps safe and consistent.</p>
                
                        <h4>How We Estimate Advantage (GAE)</h4>
                        <p>PPO often uses a trick called <strong>Generalized Advantage Estimation</strong> to reduce noise in advantage values:</p>
                        <p>
                            \[
                            \hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \cdots
                            \]
                        </p>
                        <p>Where the <em>temporal difference error</em> \(\delta_t\) is:</p>
                        <p>
                            \[
                            \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
                            \]
                        </p>
                        <ul>
                            <li><strong>V(s)</strong>: estimated value of being in state <em>s</em></li>
                            <li><strong>\(\lambda\)</strong>: parameter controlling bias vs. variance</li>
                        </ul>
                
                        <h3>How PPO is Trained</h3>
                        <ul>
                            <li>Collect trajectories (sequences of state, action, reward)</li>
                            <li>Compute:
                                <ul>
                                    <li>Total rewards (returns)</li>
                                    <li>Estimated values \(V(s_t)\)</li>
                                    <li>Advantages \(\hat{A}_t\)</li>
                                </ul>
                            </li>
                            <li>Repeat for a few training rounds:
                                <ul>
                                    <li>Use mini-batches to optimize the PPO loss</li>
                                </ul>
                            </li>
                        </ul>
                
                        <h3>Why PPO is a Good Choice</h3>
                        <ul>
                            <li><strong>Stable:</strong> Clipping prevents wild changes to the policy</li>
                            <li><strong>Efficient:</strong> You can reuse the same data for multiple updates</li>
                            <li><strong>Easy to implement:</strong> Works well with frameworks like PyTorch or TensorFlow</li>
                            <li><strong>Reliable:</strong> PPO often works "out of the box" without complex tuning</li>
                        </ul>
                    </div>
                </div>
                

                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Reward Function Design</h3>
                        <p>The reward function is a critical component in defining the agent's performance. By quantifying the current state of the agent, the reward function guides the learning process and shapes the agent's behavior. The design of the reward function is crucial for effective training and achieving desired driving behaviors.</p>
                        <p>The reward function is designed to provide feedback based on the agent's actions and the environment's state. It assigns rewards and penalties based on the agent's performance, encouraging it to learn optimal driving strategies.</p>
                        <p>The reward function is carefully designed to encourage desired driving behaviors:</p>
                        <h4>✅ Rewards</h2>
                        <table class="reward-table">
                            <thead>
                            <tr>
                                <th>Scenario</th>
                                <th>Reward Value</th>
                                <th>Purpose</th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td><strong>Forward Motion</strong></td>
                                <td><code>+0.05 × forward velocity</code></td>
                                <td>Encourages continuous forward motion</td>
                            </tr>
                            <tr>
                                <td><strong>Optimal Speed</strong><br><span class="subtext">(10–20 m/s)</span></td>
                                <td><code>+0.02</code></td>
                                <td>Incentivizes maintaining optimal speed</td>
                            </tr>
                            <tr>
                                <td><strong>Checkpoint Reached</strong></td>
                                <td><code>+2.5 + scaled bonus</code><br><span class="subtext">(based on timer ratio)</span></td>
                                <td>Strong reward with time-awareness; promotes efficiency</td>
                            </tr>
                            </tbody>
                        </table>

                        <h4>❌ Penalties</h3>
                        <table class="penalty-table">
                            <thead>
                            <tr>
                                <th>Scenario</th>
                                <th>Penalty Value</th>
                                <th>Purpose</th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td><strong>Excessive Speed</strong><br><span class="subtext">(> 20 m/s)</span></td>
                                <td><code>-0.02 × (velocity - 20)</code></td>
                                <td>Penalizes excessive speeding for each m/s over 20 m/s</td>
                            </tr>
                            <tr>
                                <td><strong>Checkpoint Timeout</strong></td>
                                <td><code>-2.0 - distance penalty</code></td>
                                <td>Encourages urgency + adds positional feedback</td>
                            </tr>
                            <tr>
                                <td><strong>Vehicle Collision</strong></td>
                                <td><code>-0.5 × cumulative reward</code></td>
                                <td>Large penalty proportional to performance; promotes cautious driving</td>
                            </tr>
                            <tr>
                                <td><strong>Wall Collision</strong></td>
                                <td><code>-0.1 × cumulative reward</code></td>
                                <td>Softens penalty if progress was made</td>
                            </tr>
                            <tr>
                                <td><strong>Wrong Checkpoint</strong></td>
                                <td><code>-1.0 - distance penalty</code></td>
                                <td>Prevents agents from driving the wrong way</td>
                            </tr>
                            </tbody>
                        </table>
                        <h4>Key notes:</h4>
                        <ul>
                            <li>Penalizing based on the cumulative reward discourages reckless behavior then a flat value</li>
                            <li>By adding an additional bonus reward based on how fast a car reaches a checkpoint encourages efficient routing and ensures that the agent continues to progress.</li>
                        </ul>
                    </div>
                </div>
                
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Multi-Agent Shared Critic</h3>
                        <h4>Architecture Overview</h4>

                        <p>
                        At the core of the shared critic architecture is a centralized value function that leverages a global state representation. Unlike traditional single-agent systems where the value function is solely based on local information, this centralized approach aggregates data from all agents, including their observations, actions, and relevant environmental signals. This global perspective enables the system to capture intricate inter-agent dependencies and emergent patterns that would otherwise be lost in decentralized learning.
                        </p>

                        <p>
                        The centralized value function is implemented using a shared neural network trained to estimate the expected return of the entire system given the global state. This value estimate reflects not just the individual contributions of each agent, but also the collective dynamics that arise from their interactions. As a result, the critic network learns to assess the overall effectiveness of coordinated behavior, making it a powerful tool for guiding policy improvement.
                        </p>

                        <p>
                        Despite the centralized nature of the training process, execution remains fully decentralized. Each agent operates based solely on its own local observations and internal policy, without needing access to the full global state or the actions of other agents at runtime. This separation ensures that agents can act independently in real-time, making the system suitable for deployment in bandwidth-constrained or communication-limited environments. To enable this, each agent is equipped with its own policy network that is trained to map its partial observations to actions. These policy networks are typically smaller and faster than the critic, optimized for real-time performance.
                        </p>

                        <p>
                        A key innovation in this architecture is the use of parameter sharing. During training, the policy networks of different agents may share weights, either fully or partially, depending on the symmetry of their roles and tasks. This strategy significantly improves learning efficiency by enabling agents to transfer knowledge gained from their individual experiences to others. For instance, an agent that learns a successful maneuver in one context can contribute to the improvement of its peers in similar situations. Furthermore, parameter sharing reduces the overall memory and computational footprint, allowing more scalable training and simpler deployment.
                        </p>

                        <h4>Training Process</h4>

                        <p>
                        The training process involves parallel experience collection from all agents interacting with a simulated or real-world environment. Each agent gathers sequences of states, actions, rewards, and next states based on its own local policy. These trajectories are then aggregated and used as input to the shared critic, which evaluates the global state and computes value estimates. These value signals serve as the baseline for computing advantages, which in turn guide policy updates.
                        </p>

                        <p>
                        Policy updates are performed using Proximal Policy Optimization (PPO), a stable and efficient policy gradient method that ensures small, incremental updates to the policy parameters. The shared critic’s value estimates are used across all agents, enabling consistent and coordinated improvement even when the agents are learning independently. This shared learning signal encourages the emergence of synergistic behaviors that benefit the entire system, rather than optimizing each agent in isolation.
                        </p>

                        <p>
                        Throughout training, parameter sharing further accelerates convergence. As agents update their shared networks, improvements discovered by one agent are immediately reflected across the others, reinforcing beneficial behaviors and reducing the need for redundant exploration. This synergy is especially valuable in complex environments where individual agents might not encounter all relevant scenarios on their own.
                        </p>

                        <h4>Advantages</h4>

                        <p>
                        One of the most significant advantages of the shared critic architecture is its sample efficiency. By aggregating experiences from multiple agents and evaluating them using a single, comprehensive critic, the system can extract more meaningful learning signals from each episode. This is particularly beneficial in sparse or delayed reward settings, where individual trajectories might offer limited feedback.
                        </p>

                        <p>
                        Another key benefit is improved generalization. Because the critic learns from a diverse range of joint states and agent configurations, it develops a more robust understanding of the environment. This enables the learned policies to generalize better to new scenarios, such as novel agent placements or unforeseen environmental conditions.
                        </p>

                        <p>
                        Training time is also significantly reduced compared to independent learning approaches. Without a shared critic, each agent must learn its own value function, leading to duplicated effort and slower convergence. By centralizing this component, the system streamlines the learning process and makes better use of computational resources.
                        </p>

                        <p>
                        Perhaps most intriguingly, the shared critic framework fosters emergent cooperative behaviors. Even without explicit coordination mechanisms, agents learn to act in ways that benefit the group as a whole because their policies are updated based on shared evaluations of joint outcomes. This implicit cooperation is a natural result of optimizing policies using a collective value signal, and it can lead to sophisticated team dynamics without the need for hand-crafted communication protocols.
                        </p>

                        <h4>Use Cases and Applicability</h4>

                        <p>
                        The shared critic architecture is particularly well-suited for domains that require both high coordination and decentralized execution. These include autonomous vehicle fleets, multi-drone navigation, robotic warehouse management, and distributed sensor systems. In these contexts, agents often operate under local constraints while contributing to a broader collective objective. The architecture’s ability to capture and exploit inter-agent synergies during training while supporting efficient individual decision-making during deployment makes it an ideal solution for such challenges.
                        </p>

                        <p>
                        Furthermore, the model’s scalability, training efficiency, and support for emergent cooperation make it a promising foundation for future developments in multi-agent reinforcement learning and collaborative AI systems.
                        </p>

                    </div>
                </div>
                
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Data Collection & Training</h3>
                        <p>The training process employs sophisticated data collection and learning strategies to ensure efficient and effective learning:</p>
                        
                        <h4>Experience Collection</h4>

                        <p>
                        A critical aspect of successful multi-agent reinforcement learning is the quality and diversity of experience collected during training. To this end, the system employs parallel environment execution, where multiple instances of the simulation environment are run concurrently. This dramatically increases the rate at which agents can collect trajectories, enabling rapid accumulation of a diverse and representative dataset. Each environment may differ slightly in its initial conditions, configurations, or stochastic elements, promoting exploration across a wider space of possible states and transitions. This parallelism not only boosts data throughput but also improves the statistical richness of the collected experience, which is essential for training generalizable policies.
                        </p>

                        <p>
                        In addition to raw throughput, the system incorporates a prioritized experience replay mechanism to focus learning on high-value experiences. Rather than treating all collected samples equally, the replay buffer assigns priority scores to each transition, typically based on the temporal-difference (TD) error observed during training. Samples with higher TD errors—often corresponding to rare events, edge cases, or difficult decision points—are more likely to be revisited during policy updates. This ensures that the learning algorithm pays special attention to scenarios where the current policy performs poorly or is uncertain, accelerating improvement in the most challenging parts of the environment. At the same time, the buffer maintains a degree of stochastic sampling to preserve coverage across the entire experience distribution and prevent overfitting to outliers.
                        </p>

                        <h4>Training Strategies</h4>

                        <p>
                        The training process is structured using curriculum learning, a technique inspired by human education systems. Rather than exposing agents to the full complexity of the environment from the outset, training begins with simple, controlled scenarios—such as navigating straight roads with minimal obstacles. These foundational tasks help agents establish basic competencies in perception, action selection, and movement coordination without being overwhelmed by noise or ambiguity.
                        </p>

                        <p>
                        As agents demonstrate proficiency in simpler tasks, the curriculum gradually increases in complexity. New challenges, such as curved roads, dynamic obstacles, intersections, or multi-agent interactions, are introduced incrementally. This progressive scaling helps agents adapt and extend their learned policies in a structured manner, reducing the risk of catastrophic forgetting and improving transfer to unseen scenarios. The curriculum can be manually defined or automatically adjusted based on performance metrics, ensuring that agents are consistently training at the edge of their competence.
                        </p>

                        <p>
                        To support effective learning throughout this staged progression, the system applies rigorous hyperparameter optimization. Key parameters such as the learning rate, batch size, and network architecture are systematically tuned to match the demands of the current learning phase. Learning rate scheduling techniques, such as cosine annealing or step decay, are used to adapt the pace of learning as agents move from exploration-heavy early stages to fine-tuning in later stages. Batch size is adjusted to balance stability and computational efficiency, while the structure of neural networks—such as the number of layers, hidden units, and activation functions—is refined to ensure sufficient model capacity without overfitting.
                        </p>

                        <p>
                        Together, these training strategies form a cohesive framework that guides agents from novice behavior to expert performance. By balancing exploration with exploitation, structuring task complexity over time, and continuously refining the learning process, the system enables the emergence of sophisticated behaviors across a wide range of operational scenarios.
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <!--
        <section id="progress">
            <div class="container">
                <h2>Project Progress & Future Plans</h2>
                <div class="progress-timeline">
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>Current Progress</h3>
                            <ul>
                                <li>Basic simulation environment setup in Unity</li>
                                <li>Initial implementation of PPO algorithm</li>
                                <li>Development of reward function framework</li>
                                <li>Basic multi-agent architecture design</li>
                                <li>Preliminary testing with simple scenarios</li>
                            </ul>
                        </div>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>Short-term Goals</h3>
                            <ul>
                                <li>Refine the reward function based on initial testing</li>
                                <li>Implement the shared critic architecture</li>
                                <li>Develop more complex road scenarios</li>
                                <li>Optimize the policy networks for better performance</li>
                                <li>Implement data collection and storage pipeline</li>
                            </ul>
                        </div>
                    </div>
                    <div class="timeline-item">
                        <div class="timeline-content">
                            <h3>Long-term Vision</h3>
                            <ul>
                                <li>Scale to handle multiple interacting vehicles</li>
                                <li>Implement advanced traffic management systems</li>
                                <li>Develop transfer learning capabilities for new scenarios</li>
                                <li>Create a comprehensive evaluation framework</li>
                                <li>Publish research findings and open-source components</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        -->

        <section id="contact">
            <div class="container">
                <h2>Get in Touch</h2>
                <div class="contact-content">
                    <p>Interested in learning more about this project or potential collaborations?</p>
                    <div class="contact-info">
                        <div class="contact-item">
                            <a href="mailto:mattusuquen@gmail.com" class="contact-button">
                                <i class="fas fa-envelope"></i>
                                <span>Email</span>
                            </a>
                        </div>
                        <div class="contact-item">
                            <a href="https://github.com/mattusuquen" target="_blank" class="contact-button">
                                <i class="fab fa-github"></i>
                                <span>GitHub</span>
                            </a>
                        </div>
                        <div class="contact-item">
                            <a href="https://www.linkedin.com/in/matthew-usuquen/" target="_blank" class="contact-button">
                                <i class="fab fa-linkedin"></i>
                                <span>LinkedIn</span>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>Developed by Matthew Usuquen.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html> 