<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autonomous Vehicle RL Project</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</head>
<body>
    <header>
        <nav>
            <div class="nav-content">
                <h1>Self-Driving Cars</h1>
                <ul>
                    <li><a href="#overview">Overview</a></li>
                    <li><a href="#demo">Demo</a></li>
                    <li><a href="#goals">Goals</a></li>
                    <li><a href="#technical">Technical Details</a></li>
                    <li class="dropdown">
                        <a href="#implementation">Implementation</a>
                        <ul class="dropdown-content">
                            <li><a href="#implementation">Environment Design</a></li>
                            <li><a href="#implementation">Agent Design</a></li>
                            <li><a href="#implementation">Proximal Policy Optimization</a></li>
                            <li><a href="#implementation">Reward Function Design</a></li>
                            <li><a href="#implementation">Multi-Agent Learning</a></li>
                            <li><a href="#implementation">Data Collection</a></li>
                        </ul>
                    </li>
                    <li><a href="#performance">Performance</a></li>
                    <li><a href="#next-steps">Next Steps</a></li>
                    <li><a href="#contact">Contact</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <main>
        <section id="hero">
            <div class="hero-content">
                <h1>Multi-Agent Reinforcement Learning for Self-Driving Cars</h1>
                <p>A sophisticated simulation environment for testing and developing autonomous driving algorithms</p>
                <a href="#overview" class="explore-button">Learn More</a>
            </div>
        </section>

        <section id="overview">
            <div class="container">
                <h2>Project Overview</h2>
                <p>This project represents an advanced exploration into autonomous vehicle navigation using reinforcement learning techniques. Through a custom-built Unity Game Engine simulation environment, I'm developing and testing sophisticated algorithms that enable autonomous vehicles to navigate complex road scenarios.</p>
                
                <div class="features">
                    <div class="feature-card">
                        <i class="fas fa-car"></i>
                        <h3>Autonomous Navigation</h3>
                        <p>Advanced algorithms for real-time decision making and control</p>
                    </div>
                    <div class="feature-card">
                        <i class="fas fa-brain"></i>
                        <h3>Reinforcement Learning</h3>
                        <p>State-of-the-art RL techniques for continuous control problems</p>
                    </div>
                    <div class="feature-card">
                        <i class="fas fa-code"></i>
                        <h3>Simulation Environment</h3>
                        <p>Modular Unity Game Engine-based testing environment</p>
                    </div>
                </div>
            </div>
        </section>
        <section id="demo">
            <div class="container">
                <h2>Project Demo</h2>
                <p>Below are live demonstrations of the multi-agent reinforcement learning system in action. These examples showcase autonomous vehicles navigating different road scenarios, executing turns, avoiding collisions, and reacting to dynamic environments.</p>
                
                <div class="demo-gallery">
        
                    <div class="demo-item">
                        <h3>Multi-Agent Coordination</h3>
                        <video autoplay muted loop>
                            <source src="videos/demo 3.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>Multiple agents navigating the environment with decentralized policies but trained with a shared critic.</p>
                    </div>
                    
                    <div class="demo-item">
                        <h3>Shared Critic Behavior</h3>
                        <video autoplay muted loop>
                            <source src="videos/demo 2.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                        <p>However in a different scenario due to the shared critic, agents learned to push each other to get to checkpoints faster.</p>
                    </div>
                </div>
            </div>
        </section>
        <section id="goals">
            <div class="container">
                <h2>Project Goals</h2>
                <div class="goals-grid">
                    <div class="goal-item">
                        <h3>Technical Understanding</h3>
                        <p>Develop comprehensive understanding of reinforcement learning techniques applied to continuous control problems</p>
                    </div>
                    <div class="goal-item">
                        <h3>Simulation Development</h3>
                        <p>Create a flexible and modular simulation environment using Unity Game Engine</p>
                    </div>
                    <div class="goal-item">
                        <h3>Algorithm Demonstration</h3>
                        <p>Demonstrate the capabilities of Multi-Agent Proximal Policy Optimization in autonomous vehicle navigation</p>
                    </div>
                    <div class="goal-item">
                        <h3>Research Foundation</h3>
                        <p>Establish a foundation for further research and development in autonomous driving</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="technical">
            <div class="container">
                <h2>Technical Stack</h2>
                <div class="tech-stack">
                    <div class="tech-item">
                        <i class="fab fa-python"></i>
                        <h3>Python</h3>
                        <p>Primary programming language</p>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-gamepad"></i>
                        <h3>Unity Game Engine</h3>
                        <p>Simulation environment development</p>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-network-wired"></i>
                        <h3>PyTorch/TensorFlow</h3>
                        <p>Reinforcement learning implementation</p>
                    </div>
                    <div class="tech-item">
                        <i class="fas fa-book"></i>
                        <h3>Jupyter Notebook</h3>
                        <p>Documentation and experimentation</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="implementation">
            <div class="container">
                <h2>Implementation Details</h2>
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Environment Design</h3>
                        <p>
                            The simulation was built using Unity's ML-Agents toolkit to provide a controlled and flexible environment for reinforcement learning experiments. The enviornment is optimized to ensure a balance between realism and performance, especially when having to deal with multithreading these instances.
                          </p>
                        
                          <h4 style="margin-top: 1.5rem;">Key Features:</h4>
                          <ul style="line-height: 1.6;">
                            <li>
                              <strong>Track Layouts:</strong> The environment includes multiple road configurations such as straight segments, sharp turns, intersections, and continuous loops. These variations are designed to challenge the agent's ability to generalize its learned policies across different driving conditions. In order to ensure smooth training, the agent is initially trained on the race track on the left to learn the basic behaviors required to drive. Then the pretrained agent is transfered to the urban environment featured on the left to learn how to interact with sharper turns and intersections.
                              <ul>
                                <li><img src="images/env-1.png"></li>
                                <li><img src="images/env-2.png"></li>
                              </ul>
                            </li>
                            <li>
                              <strong>Road Markings & Boundaries:</strong> Lanes are marked with distinct colors and borders to simulate real-world road constraints. Going off-road or crossing into restricted areas results in negative rewards, guiding the agent to stay within safe driving zones. When the agent enters the red areas, the agent is penalized. The green represents the checkpoints to guide the direction of the agent. The agent is awarded when reaching the correct checkpoint.
                              <ul>
                                <li><img src="images/boundaries.png"></li>
                              </ul>
                            </li>
                            <li>
                              <strong>Sensor System:</strong> The agent is equipped with a omni-directional raycast sensor array that simulates lidar-like perception. These rays detect distances to walls and obstacles, feeding spatial data into the neural network to inform decision-making. Note that the checkpoints are not detected by the sensors.
                              <ul>
                                <li>
                                    <video autoplay muted loop>
                                    <source src="videos/sensor.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                    </video>
                                </li>
                              </ul>
                            </li>
                            <li>
                              <strong>Action Space:</strong> The agent outputs continuous values for steering (negative values representing breaking) and throttle to act as if the agent is actually turning the wheel and stepping on the pedal. This enables smoother and more realistic driving behavior compared to discrete action spaces because it can make small fluid adjustments instead of being limited to a set of actions. In a complex driving environment, having a continous action space allows the agent to make detailed adjustments when dealing with avoiding aobstacles. However, this comes with the drawback of a longer training time because of the infinite number of possible actions that the agent needs to explore and learn causing noisier updates and slower convergance. In order to train on a continuous action the dataset must include unique states so that the agent can learn the proper behaviour for these states.
                            </li>
                            <li>
                              <strong>Reset Conditions:</strong> Episode resets are useful when defining the end state of the simulation. On reset, the cars are sent to there starting positions with added positional and rotational noise to further diversify the dataset. Episodes are reset if the following conditions are met:
                                <ul>
                                    <li>When the agent is collided with a car or wall for a period of time. Instead of resetting on collision, we can artificially extending the agents episode length, allowing the agent to learn the penalties associated with crashing and giving it the ability to recover.</li>
                                    <li>Hitting the wrong checkpoint. This ensures the car is going in the correct direction.</li>
                                    <li>Time limit has been reached before reaching the next checkpoint. This makes it so agents are not stalling.</li>
                                </ul>
                            </li>
                          </ul>
                    </div>
                </div>

                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Agent Design</h3>
                        <p>The agent's architecture and components are carefully designed to process environmental information and make appropriate driving decisions.</p>
                        <h4>Key Components:</h4>
                        <ul>
                            <li>
                                <strong>Controller & Physics System:</strong> The agent uses Unity's physics engine to create a Rigidbody to simulate realistic movement, acceleration, braking, and steering:
                                <ul>
                                    <li>Motor torque is applied to the rear wheels based on gas input and engine power</li>
                                    <li>Torque is calculated from the engine RPM and power curve</li>
                                    <li>Gear shifting: upshift when RPM exceeds a threshold and downshift when RPM drops below a threshold</li>
                                    <li>Brake torque is applied to all wheels</li>
                                </ul>
                            </li>
                            <li>
                                <strong>Observation Space:</strong> The agent receives a comprehensive set of observations including:
                                <ul>
                                    <li>Raycast sensor data for obstacle detection</li>
                                    <li>Global position</li>
                                    <li>Normalized rotational vector</li>
                                    <li>Velocity vector and magnitude</li>
                                    <li>Angular vector and magnitude</li>
                                    <li>Checkpoint position</li>
                                </ul>
                            </li>
                            <li>
                                <strong>State Stacking & State Prediction:</strong>
                                <p>The agent's decision-making process is enhanced through a sophisticated state stacking and prediction mechanism. This approach allows the agent to maintain temporal awareness window and make more informed decisions by considering both historical and predicted future states.</p>

                                <h4>State Stacking</h4>
                                <p>The state stacking mechanism concatenates the current state with the previous two states to create a temporal context for decision-making. This approach is crucial for several reasons:</p>
                                <ul>
                                    <li>The current state provides immediate information about the agent's position, velocity, and surrounding environment.</li>
                                    <li>The previous two states help the agent understand its motion dynamics, including acceleration patterns and trajectory trends.</li>
                                    <li>This temporal context enables the agent to make smoother decisions by considering how its state has evolved over time.</li>
                                </ul>
                                <p>For example, when approaching a turn, the stacked states help the agent understand its current speed, how it has been accelerating or decelerating, and what adjustments might be needed to navigate the turn smoothly.</p>

                                <h4>State Prediction</h4>
                                <p>The state prediction system uses a pretrained neural network to forecast the next state given the current state. This prediction network can then be applied on the predicted state to get the second predicted state. The process can be repeated to achieve a prediction of the next k states, but will start to see drops in accuracy.</p>
                                <ul>
                                    <li>The pretrained network has learned the dynamics of the environment and can accurately predict how the agent's state will change.</li>
                                    <li>By predicting future states, the agent can anticipate potential issues before they occur, such as upcoming obstacles or the need to adjust speed for a turn.</li>
                                    <li>The predictions help the agent make more proactive decisions rather than purely reactive ones.</li>
                                </ul>
                                <p>The prediction network takes as input the current state and the stacked historical states, and outputs predictions for the next two timesteps. These predictions include expected positions, velocities, and other relevant state variables. This forward-looking capability is particularly valuable in dynamic environments where immediate reactions might be too late to avoid problems.</p>

                                <h4>Integration and Benefits</h4>
                                <p>The combination of state stacking and prediction creates a powerful decision-making framework:</p>
                                <ul>
                                    <li><strong>Enhanced Temporal Awareness:</strong> The agent maintains a clear understanding of both its past trajectory and likely future states, enabling more informed decision-making.</li>
                                    <li><strong>Proactive Planning:</strong> By predicting future states, the agent can plan its actions in advance rather than simply reacting to immediate circumstances.</li>
                                    <li><strong>Improved Stability:</strong> The temporal context helps prevent erratic behavior by considering the agent's motion history when making decisions.</li>
                                    <li><strong>Better Performance:</strong> This approach leads to smoother trajectories, more efficient navigation, and better handling of complex scenarios.</li>
                                </ul>
                                <p>For instance, when navigating through a series of turns, the agent can use its stacked states to understand its current trajectory and use the predicted states to plan the necessary adjustments for upcoming turns. This results in smoother, more efficient navigation compared to making decisions based solely on the current state.</p>
                            </li>
                            <li>
                                <strong>Neural Network Architecture:</strong>
                                <ul>
                                    <li>Input layer processes sensor and state data</li>
                                    <li>Multiple hidden layers with ReLU activation</li>
                                    <li>Output layer for continuous action values for gas/brake, and steering angle</li>
                                    <li>Dropout layers and normalization used to prevent overfitting</li>
                                </ul>
                            </li>
                        </ul>
                    </div>
                </div>
                
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Proximal Policy Optimization (PPO)</h3>
                        <p>PPO is a popular algorithm in reinforcement learning. It helps train an agent (like a robot or game player) to make good decisions by improving its strategy little by little in a stable way. PPO works well because it keeps policy updates from being too big, which can otherwise break learning.</p>
                
                        <h4>What’s the Goal in Reinforcement Learning?</h4>
                        <p>In RL, we want to find the best <strong>policy</strong>—a function that tells our agent what to do in each situation (state) to get the most reward over time.</p>
                        <p>The expected reward is written like this:</p>
                        <p>
                            \[
                            J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]
                            \]
                        </p>
                        <ul>
                            <li><strong>s</strong>: current state</li>
                            <li><strong>a</strong>: action taken</li>
                            <li><strong>r_t</strong>: reward at time <em>t</em></li>
                            <li><strong>\(\gamma\)</strong>: discount factor (makes future rewards worth slightly less)</li>
                            <li><strong>\(\pi_\theta(a|s)\)</strong>: the policy’s probability of choosing action <em>a</em> in state <em>s</em></li>
                        </ul>
                
                        <h4>Basic Policy Gradient Method</h4>
                        <p>The basic way to improve the policy is called the <strong>policy gradient</strong>. It tries to push the policy in a direction that increases expected reward. The formula looks like this:</p>
                        <p>
                            \[
                            \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot A^{\pi}(s,a) \right]
                            \]
                        </p>
                        <p>Here, \( A^{\pi}(s,a) \) is the <em>advantage</em>, which tells us how much better an action is than average at a given state.</p>
                        <p>The problem is: basic policy gradients can make changes that are too big, which makes learning unstable.</p>
                
                        <h4>PPO: A Smarter Way to Update the Policy</h4>
                
                        <h5>Step 1: Compare New and Old Policy</h5>
                        <p>PPO compares the new and old policy using this ratio:</p>
                        <p>
                            \[
                            r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}
                            \]
                        </p>
                        <p>This tells us how much the new policy differs from the old one for the same action.</p>
                
                        <h5>Step 2: Clip the Objective</h5>
                        <p>Instead of letting this ratio get too big or too small, we clip it. The goal is to avoid big, risky updates.</p>
                        <p>
                            \[
                            L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
                            \]
                        </p>
                        <ul>
                            <li>\(\hat{A}_t\): advantage at time <em>t</em></li>
                            <li>\(\epsilon\): small constant (usually 0.1 or 0.2)</li>
                        </ul>
                        <p>This keeps the update close to the old policy, which makes learning more stable.</p>
                
                        <h5>Why Clipping Helps</h5>
                        <p>If the ratio becomes too large or too small (e.g., 1.5 or 0.5), clipping stops the algorithm from giving it too much weight. This keeps learning steps safe and consistent.</p>
                
                        <h4>How We Estimate Advantage (GAE)</h4>
                        <p>PPO often uses a trick called <strong>Generalized Advantage Estimation</strong> to reduce noise in advantage values:</p>
                        <p>
                            \[
                            \hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \cdots
                            \]
                        </p>
                        <p>Where the <em>temporal difference error</em> \(\delta_t\) is:</p>
                        <p>
                            \[
                            \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
                            \]
                        </p>
                        <ul>
                            <li><strong>V(s)</strong>: estimated value of being in state <em>s</em></li>
                            <li><strong>\(\lambda\)</strong>: parameter controlling bias vs. variance</li>
                        </ul>
                
                        <h3>How PPO is Trained</h3>
                        <ul>
                            <li>Collect trajectories (sequences of state, action, reward)</li>
                            <li>Compute:
                                <ul>
                                    <li>Total rewards (returns)</li>
                                    <li>Estimated values \(V(s_t)\)</li>
                                    <li>Advantages \(\hat{A}_t\)</li>
                                </ul>
                            </li>
                            <li>Repeat for a few training rounds:
                                <ul>
                                    <li>Use mini-batches to optimize the PPO loss</li>
                                </ul>
                            </li>
                        </ul>
                
                        <h3>Why PPO is a Good Choice</h3>
                        <ul>
                            <li><strong>Stable:</strong> Clipping prevents wild changes to the policy</li>
                            <li><strong>Efficient:</strong> You can reuse the same data for multiple updates</li>
                            <li><strong>Easy to implement:</strong> Works well with frameworks like PyTorch or TensorFlow</li>
                            <li><strong>Reliable:</strong> PPO often works "out of the box" without complex tuning</li>
                        </ul>
                    </div>
                </div>
                

                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Reward Function Design</h3>
                        <p>The reward function is used to understand the value of each state. By quantifying the current state of the agent, the reward function guides the learning process and shapes the agent's behavior. The design of the reward function is crucial for effective training and achieving desired driving behaviors.</p>
                        <p>The reward function is designed to provide feedback based on the agent's actions and the environment's state. It assigns rewards and penalties based on the agent's performance, encouraging it to learn optimal driving strategies.</p>
                        <p>The reward function is carefully designed to encourage desired driving behaviors:</p>
                        <h4>✅ Rewards</h2>
                        <table class="reward-table">
                            <thead>
                            <tr>
                                <th>Scenario</th>
                                <th>Reward Value</th>
                                <th>Purpose</th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td><strong>Forward Motion</strong></td>
                                <td><code>+0.05 × forward velocity</code></td>
                                <td>Encourages continuous forward motion</td>
                            </tr>
                            <tr>
                                <td><strong>Optimal Speed</strong><br><span class="subtext">(10–20 m/s)</span></td>
                                <td><code>+0.02</code></td>
                                <td>Incentivizes maintaining optimal speed</td>
                            </tr>
                            <tr>
                                <td><strong>Checkpoint Reached</strong></td>
                                <td><code>+2.5 + scaled bonus</code><br><span class="subtext">(based on timer ratio)</span></td>
                                <td>Strong reward with time-awareness; promotes efficiency</td>
                            </tr>
                            </tbody>
                        </table>

                        <h4>❌ Penalties</h3>
                        <table class="penalty-table">
                            <thead>
                            <tr>
                                <th>Scenario</th>
                                <th>Penalty Value</th>
                                <th>Purpose</th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td><strong>Excessive Speed</strong><br><span class="subtext">(> 20 m/s)</span></td>
                                <td><code>-0.02 × (velocity - 20)</code></td>
                                <td>Penalizes excessive speeding for each m/s over 20 m/s</td>
                            </tr>
                            <tr>
                                <td><strong>Checkpoint Timeout</strong></td>
                                <td><code>-2.0 - distance penalty</code></td>
                                <td>Encourages urgency + adds positional feedback</td>
                            </tr>
                            <tr>
                                <td><strong>Vehicle Collision</strong></td>
                                <td><code>-0.5 × cumulative reward</code></td>
                                <td>Large penalty proportional to performance; promotes cautious driving</td>
                            </tr>
                            <tr>
                                <td><strong>Wall Collision</strong></td>
                                <td><code>-0.1 × cumulative reward</code></td>
                                <td>Softens penalty if progress was made</td>
                            </tr>
                            <tr>
                                <td><strong>Wrong Checkpoint</strong></td>
                                <td><code>-1.0 - distance penalty</code></td>
                                <td>Prevents agents from driving the wrong way</td>
                            </tr>
                            </tbody>
                        </table>
                        <h4>Key notes:</h4>
                        <ul>
                            <li>Penalizing based on the cumulative reward discourages reckless behavior then a flat value</li>
                            <li>By adding an additional bonus reward based on how fast a car reaches a checkpoint encourages efficient routing and ensures that the agent continues to progress.</li>
                        </ul>
                    </div>
                </div>
                
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Multi-Agent Learning</h3>
                        <h4>Architecture Overview</h4>

                        <p>
                        At the core of the shared critic architecture is a centralized value function that leverages a global state representation. Unlike traditional single-agent systems where the value function is solely based on local information, this centralized approach aggregates data from all agents, including their observations, actions, and relevant environmental signals. This global perspective enables the system to capture intricate agent interactions and emergent patterns that would otherwise be lost in decentralized learning.
                        </p>

                        <p>
                        The centralized value function is implemented using a shared neural network trained to estimate the expected return of the entire system given the global state. This value estimate reflects not just the individual contributions of each agent, but also the collective dynamics that arise from their interactions. As a result, the critic network learns to assess the overall effectiveness of coordinated behavior, making it a powerful tool for guiding policy improvement.
                        </p>

                        <p>
                        Despite the centralized nature of the training process, execution remains fully decentralized. Each agent operates based solely on its own local observations and internal policy, without needing access to the full global state or the actions of other agents at runtime. This separation ensures that agents can act independently in real-time, making the system suitable for deployment in bandwidth-constrained or communication-limited environments. To enable this, each agent is equipped with its own policy network that is trained to map its partial observations to actions. 
                        </p>

                        <p>
                        This architecture utilizes parameter sharing. During training, the policy networks of different agents may share weights, either fully or partially, depending on the symmetry of their roles and tasks. This strategy significantly improves learning efficiency by enabling agents to transfer knowledge gained from their individual experiences to others. For instance, an agent that learns a successful maneuver in one context can contribute to the improvement of its peers in similar situations. Furthermore, parameter sharing reduces the overall memory and computational footprint, allowing more scalable training which will be important when we get to multithreading.
                        </p>

                        <h4>Training Process</h4>

                        <p>
                        The training process involves parallel experience collection from all agents interacting with a simulated or real-world environment. Each agent gathers sequences of states, actions, rewards, and next states based on its own local policy. These trajectories are then aggregated and used as input to the shared critic, which evaluates the global state and computes value estimates. These value signals serve as the baseline for computing advantages, which in turn guide policy updates.
                        </p>

                        <p>
                        Policy updates are performed using Proximal Policy Optimization (PPO). The shared critic’s value estimates are used across all agents, enabling consistent and coordinated improvement even when the agents are learning independently. This shared learning signal encourages the emergence of synergistic behaviors that benefit the entire system, rather than optimizing each agent in isolation.
                        </p>

                        <p>
                        Throughout training, parameter sharing further accelerates convergence. As agents update their shared networks, improvements discovered by one agent are immediately reflected across the others, reinforcing beneficial behaviors and reducing the need for redundant exploration. This synergy is especially valuable in complex environments where individual agents might not encounter all relevant scenarios on their own.
                        </p>

                        <h4>Advantages</h4>

                        <p>
                        One of the most significant advantages of the shared critic architecture is its sample efficiency. By aggregating experiences from multiple agents and evaluating them using a single, comprehensive critic, the system can extract more meaningful learning signals from each episode. This is particularly beneficial in sparse or delayed reward settings, where individual trajectories might offer limited feedback.
                        </p>

                        <p>
                        Another key benefit is improved generalization. Because the critic learns from a diverse range of joint states and agent configurations, it develops a more robust understanding of the environment. This enables the learned policies to generalize better to new scenarios, such as novel agent placements or unforeseen environmental conditions.
                        </p>

                        <p>
                        Training time is also significantly reduced compared to independent learning approaches. Without a shared critic, each agent must learn its own value function, leading to duplicated effort and slower convergence. By centralizing this component, the system streamlines the learning process and makes better use of computational resources.
                        </p>

                        <p>
                        The shared critic framework fosters emergent cooperative behaviors. Even without explicit coordination mechanisms within the loss or reward function, agents learn to act in ways that benefit the group as a whole because their policies are updated based on shared evaluations of joint outcomes. This implicit cooperation is a natural result of optimizing policies using a collective value signal, and it can lead to sophisticated team dynamics without the need for hand-crafted communication protocols.
                        </p>
                    </div>
                </div>
                
                <div class="implementation-content">
                    <div class="implementation-text">
                        <h3>Data Collection & Training</h3>
                        <p>The training process employs sophisticated data collection and learning strategies to ensure efficient and effective learning:</p>
                        
                        <h4>Experience Collection</h4>

                        <p>
                        An agent can only learn what it's seen before. Without a diverse dataset, the agent will be unable to learn the desired behavior. To combat this the system employs parallel environment execution, where multiple instances of the simulation environment are run concurrently. This dramatically increases the rate at which agents can collect trajectories, enabling rapid accumulation of a diverse and representative dataset. Each environment may differ slightly in its initial conditions, like number of agents. These initial conditions are randomized and allows for each instance to be completely unique from one another to romote explorating across a wider space of possible states and transitions. This parallelism not only boosts data throughput, but fills the dataset with unique states. Which is essential for training generalizable policies.
                        </p>

                        <h4>Training Strategies</h4>

                        <p>
                        The training process is structured using curriculum learning, a technique inspired by human education systems. Rather than exposing agents to the full complexity of the environment from the outset, training begins with simple, controlled scenarios. This agent started with the task of navigating straight roads with no other agents. These foundational tasks help agents establish basic competencies in perception, action selection, and movement coordination without being overwhelmed by the complexity that comes with added features.
                        </p>

                        <p>
                        As agents demonstrate proficiency in simpler tasks, the curriculum gradually increases in complexity. New challenges, such as curved roads, dynamic obstacles, intersections, or multi-agent interactions, are introduced incrementally. This progressive scaling helps agents adapt and extend their learned policies in a structured manner, reducing the risk of catastrophic forgetting and improving transfer to unseen scenarios. The curriculum can be manually defined or automatically adjusted based on performance metrics, ensuring that agents are consistently training at the edge of their competence.
                        </p>
                        <p>
                        By balancing exploration with exploitation, structuring task complexity over time, and continuously refining the learning process, the system enables the emergence of sophisticated behaviors across a wide range of scenarios.
                        </p>
                    </div>
                </div>
            </div>
        </section>
        <section id="performance">
            <div class="container">
                <h2>Performance Analysis</h2>
                <p>This section presents the training results and performance benchmarks of the autonomous driving agents. Various metrics such as reward accumulation, policy loss, and success rates are visualized and analyzed to demonstrate learning progress and stability.</p>
        
                <div class="performance-gallery">
        
                    <div class="performance-item">
                        <h3>Average Cumulative Reward Over Time</h3>
                        <img src="images/cumulative reward.png" alt="Average Cumulative Reward per Episode Graph">
                        <ul>
                            <li>The cumulative reward graph tracks the total reward accumulated over time.</li>
                            <li>The reward starts at a low value due to its randomized initialization, but it consistently increases as the simulation progresses, indicating that the agents are getting better at completing tasks and optimizing the reward function.</li>
                        </ul>
                    </div>
        
                    <div class="performance-item">
                        <h3>Episode Duration</h3>
                        <img src="images/episode length.png" alt="Loss Curves">
                        <p>This graph shows how the episode length changes over time as the simulation progresses and is used to describe the average survival length of the agents. The episode length steadily increases as the simulation advances, indicating that the agents are learning how to survive longer within the environment.</p>
                    </div>
                    <div class="performance-item">
                        <h3>Velocity Distribution</h3>
                        <img src="images/max velocity.png" alt="Max Velocity over Time">
                        <img src="images/avg velocity.png" alt="Average Velocity over Time">
                        <img src="images/min velocity.png" alt="Min Velocity over Time">
                        <p>The distribution reflects that the agent has learned to maintain speeds within the optimal range. The presence of excessive speeds decreases significantly as penalties shape the agent’s behavior. Also, we can note that there is an increase in speed over time, showing how the agent is able to survive longer lengths while maintaining a faster velocity.</p>
                    </div>
        
                </div>
            </div>
        </section>
        <section id="next-steps">
            <div class="container">
                <h2>Next Steps</h2>
                <p>This project is continually evolving. Below are the planned next steps to enhance functionality, improve performance, and extend the scope of the research.</p>
        
                <div class="next-steps-grid">
                    <div class="next-step-item">
                        <h3>Convolutional Neural Networks</h3>
                        <p>Utilizing CNNs would allow the ability to capture local features and find relationships between the stacked states.</p>
                        
                    </div>
        
                    <div class="next-step-item">
                        <h3>Experiment with Advanced Environments</h3>
                        <p>Incorporate designated lanes and traffic lights to get the agent to learn difficult situations like merging or lane changing.</p>
                    </div>
        
                    <div class="next-step-item">
                        <h3>Advanced Sensor Modeling</h3>
                        <p>Simulate lidar, gps, camera feeds to create more realistic observation spaces and test what sensor technique performs the best.</p>
                    </div>
        
                    <div class="next-step-item">
                        <h3>Cross-Agent Communication</h3>
                        <p>Experiment with explicit message passing between agents to improve coordination in complex multi-agent environments such as intersections or roundabouts.</p>
                    </div>
                </div>
            </div>
        </section>
        

        <section id="contact">
            <div class="container">
                <h2>Get in Touch</h2>
                <div class="contact-content">
                    <p>Interested in learning more about this project or potential collaborations?</p>
                    <div class="contact-info">
                        <div class="contact-item">
                            <a href="mailto:mattusuquen@gmail.com" class="contact-button">
                                <i class="fas fa-envelope"></i>
                                <span>Email</span>
                            </a>
                        </div>
                        <div class="contact-item">
                            <a href="https://github.com/mattusuquen" target="_blank" class="contact-button">
                                <i class="fab fa-github"></i>
                                <span>GitHub</span>
                            </a>
                        </div>
                        <div class="contact-item">
                            <a href="https://www.linkedin.com/in/matthew-usuquen/" target="_blank" class="contact-button">
                                <i class="fab fa-linkedin"></i>
                                <span>LinkedIn</span>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>Developed by Matthew Usuquen.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html> 